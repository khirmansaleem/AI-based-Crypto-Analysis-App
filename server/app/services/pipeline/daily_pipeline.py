from app.database.database import SessionLocal
import logging

# from app.services.ai.backfill_embedding_core import backfill_embeddings_core
# from app.services.news.importer import import_scraped_articles_core
from app.models.news.news_article import NewsArticle
from app.services.pipeline.process_article import process_article
from app.scrapers.cryptoslate_scraper.scraper import scrape_latest_news
from app.services.ai.backfill_embedding_core import backfill_embeddings_core
from app.services.news.importer import import_scraped_articles_core


logger = logging.getLogger(__name__)


async def process_daily_news():
    """
    This function is called daily by the scheduler.
    It handles the full daily pipeline:
    1. Import scraped TXT files
    2. Generate embeddings
    3. Analyze unprocessed articles
    """

    logger.info("ğŸš€ Daily News Pipeline Started")

    # ---------------------------
    # STEP 0 â€” SCRAPE LATEST NEWS
    # ---------------------------

    try:
        logger.info("ğŸ•¸ Starting scraper...")
        count = scrape_latest_news()
        logger.info(f"ğŸ“° Scraper finished â€” {count} new articles saved as TXT")
    except Exception as e:
        logger.error(f"âŒ Scraper failed: {e}")
        # We continue â€” maybe older TXT still exist

    db = SessionLocal()

    try:
        # STEP 1 â€” Import scraped articles
        logger.info("ğŸ“¥ Importing scraped articles...")
        await import_scraped_articles_core(db)

        # STEP 2 â€” Generate embeddings for new articles
        logger.info("ğŸ§  Generating embeddings...")
        backfill_embeddings_core()

        # STEP 3 â€” Fetch all articles that are not analyzed yet
        new_articles = (
            db.query(NewsArticle).filter(NewsArticle.is_analyzed == False).all()
        )

        logger.info(f"ğŸ“ Found {len(new_articles)} new articles to analyze.")

        # STEP 4 â€” Process each article
        for article in new_articles:
            logger.info(f"ğŸ” Processing article ID={article.id}")
            await process_article(db, article.id)

        logger.info("âœ… Daily News Pipeline Completed Successfully")

    except Exception as e:
        logger.error(f"âŒ Error in daily pipeline: {e}", exc_info=True)

    finally:
        db.close()
